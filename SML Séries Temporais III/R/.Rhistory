model = "AAA",
gamma = 0.21)
qcement.f6 <- forecast(qcement.hw6,
h = 5)
accuracy(qcement.f6, qcement.test)
# Valores preditos
qcement.f6
autoplot(qcement.f6)
# Acessando a base do pacote "ggseas"
nzdata <- data.table(nzbop)
nzdata <- nzdata[!((Account=="Capital account"&
Category=="Balance")|
(Account=="Financial account"&
Category=="Foreign inv. in NZ; Financial derivative liabilities")|
(Category=="Secondary income balance")),]
sample_ts <- nzdata[Account == "Current account" & Category=="Services; Exports total",
.(TimePeriod, Value)]
kable(head(sample_ts)) # visualização da base
# Extraindo a tendência, pode ser feito calculando a média móvel ou mediana.
# Uma mediana móvel é menos sensível a outliers do que uma média móvel
sample_ts[, trend := rollmean(Value, 8, fill=NA, align = "right")]
kable(tail(sample_ts))
# Redução da Tendência
# A redução precisa ser aditiva ou multiplcativa, como ainda não sabemos
# faremos os dois processos.
sample_ts[,`:=`( detrended_a = Value - trend,  detrended_m = Value / trend )]
kable(tail(sample_ts))
# Sazonalidade
# É preciso avaliar os valores típicos de tendência ao longo do ciclo.
# A título de aprendizado vamos calcular o valor médio das observações
# em Q1, Q2, Q3 e Q4.
sample_ts[,`:=`(seasonal_a = mean(detrended_a, na.rm = TRUE),
seasonal_m = mean(detrended_m, na.rm = TRUE)),
by=.(quarter(TimePeriod)) ]
kable(tail(sample_ts))
# Com a tendência e a sazonalidade é possível calcular os resíduos.
sample_ts[,`:=`( residual_a = detrended_a - seasonal_a,
residual_m = detrended_m / seasonal_m )]
kable(tail(sample_ts))
# Modelo Aditivo
ggsdc(sample_ts, aes(x = TimePeriod, y = Value), method = "decompose",
frequency = 4, s.window = 8, type = "additive")+ geom_line()+
ggtitle("Additive")+ theme_minimal()
# Modelo Multiplicativo
ggsdc(sample_ts, aes(x=TimePeriod, y=Value), method = "decompose",
frequency=4, s.window=8, type = "multiplicative")+ geom_line()+
ggtitle("Multiplicative")+ theme_minimal()
ssacf <- function(x) sum(acf(x, na.action = na.omit)$acf^2)
compare_ssacf <-function(add,mult) ifelse(ssacf(add)< ssacf(mult),
"Additive", "Multiplicative")
kable(sample_ts[,.(compare_ssacf(residual_a, residual_m ))])
pacotes <- c("forecast","tidyverse","prophet")
if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
instalador <- pacotes[!pacotes %in% installed.packages()]
for(i in 1:length(instalador)) {
install.packages(instalador, dependencies = T)
break()}
sapply(pacotes, require, character = T)
} else {
sapply(pacotes, require, character = T)
}
# Criando a base de dados
values <- c(92.1,  92.6,  89.5,  80.9,  95.6,  72.5,  71.2,  78.8,  73.8,  83.5,
97.9, 93.4,  98.0,  90.2,  96.7, 100.0, 103.6,  74.6,  78.9,  92.0,
83.4,  98.1, 109.9, 102.2, 102.1,  96.2, 106.9,  95.1, 113.4,  84.0,
88.6,  94.9,  94.7, 105.7, 108.6, 101.9,  113.9, 100.9, 100.2,  91.9,
99.6,  87.2,  92.1, 104.9, 103.4, 103.3, 103.9, 108.5)
# Formatando como Série Temporal
time_series <- ts(values, start = 2015, frequency =12)
# Decomposição
autoplot(decompose(time_series)) + ggtitle("Decomposition of the series") +
theme(plot.title = element_text(size=8))
# ARIMA
forecast_arima <- auto.arima(time_series, seasonal=TRUE, stepwise = FALSE, approximation = FALSE)
# Caso a tenha problema com a linha 513, reinstale o pacote "forecast"
# E claro, ative o pacote novamente para seguir.
# O, eventual, erro se refere a dependência com pacotes similares de previsão
forecast_arima = forecast(forecast_arima, h=60)
plot(forecast_arima)
# ARIMA
forecast_arima <- auto.arima(time_series, seasonal=TRUE, stepwise = FALSE, approximation = FALSE)
# Caso a tenha problema com a linha 513, reinstale o pacote "forecast"
# E claro, ative o pacote novamente para seguir.
# O, eventual, erro se refere a dependência com pacotes similares de previsão
forecast_arima = forecast(forecast_arima, h=60)
plot(forecast_arima)
# Holt-Winters
forecast_hw <- hw(time_series, seasonal="multiplicative", h=60)
summary(forecast_hw) # avaliando os resultados
plot(forecast_hw) # plotando os resultados
# ARIMA
autoplot(time_series, series=" Historical data") +
autolayer(forecast_arima, series=" ARIMA Forecast") +
ggtitle(" ARIMA forecasting") +
theme(plot.title = element_text(size=8))
# Holt-Winters
autoplot(time_series, series=" Historical data") +
autolayer(forecast_hw, series="Holt-Winter forecast") +
ggtitle("HW Exponential Smoothing") +
theme(plot.title = element_text(size=8))
# Modelo de previsão
forecast_arima['model'] #ARIMA
forecast_hw['model'] #Holt Winter
# Acurácia do Modelo
accuracy(forecast_arima) #ARIMA
accuracy(forecast_hw) #Holt Winter
# 1. Explorando os dados
load("lebron.RData") # carregando a base
load("~/GitHub/DSA211/SML Séries Temporais III/R/lebronRData Portugues.RData")
colnames(lebron) <- c("ds", "y")  # renomeando as colunas
head(lebron) # visualizando a base
lebron$y <- log10(lebron$y) # aplicando logaritmo base 10 na variável y
View(summary(lebron)) # explorando os dados
plot(y ~ ds, lebron, type = "l") # gráfico da série
# 2. Predições Básicas
m <- prophet(lebron)
future <- make_future_dataframe(m, periods = 365)
forecast <- predict(m, future)
plot(m, forecast) # visualização simples para compreensão
# Avaliação por valores brutos com o valor previsto por dia e intervalos de incerteza
tail(forecast[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')])
tail(forecast) # previsão por componentes
# 3. Inspecionando Componentes do Modelo
prophet_plot_components(m, forecast)
data(AirPassengers)
AirPassengers
plot(AirPassengers, ylab="Passengers", type="o", pch =20) # visualizando a série
# Separação entre treino e teste (período de dois anos)
df_train<- window(AirPassengers, end = c(1958, 12))
df_test <- window(AirPassengers, start = c(1959, 01))
ggtsdisplay(diff(log(AirPassengers), 12)) # avaliação da autocorrelação
arima_1 <- Arima(df_train, c(2, 0, 0), c(0, 1, 1), include.constant = TRUE, lambda = 0)
ggtsdisplay(arima_1$residuals)
arima_f <- forecast(arima_1, 24)
forecast(arima_1, 24) %>% autoplot()
err = df_test - arima_f$mean
mape <- mean(abs(err) / (arima_f$mean+ err)) * 100
rmse <- sqrt(mean(err^2, na.rm = TRUE))
mae <- mean(abs(err), na.rm = TRUE)
cbind(mape, rmse, mae)
pacotes <- c("prophet","tidymodels","fpp2","tidyverse","tseries","itsmr",
"timetk","modeltime","glmnet","h2o","modeltime.h2o")
if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
instalador <- pacotes[!pacotes %in% installed.packages()]
for(i in 1:length(instalador)) {
install.packages(instalador, dependencies = T)
break()}
sapply(pacotes, require, character = T)
} else {
sapply(pacotes, require, character = T)
}
install.packages(instalador, dependencies = T)
load("air_passengers.RData")
load("~/GitHub/DSA211/SML Séries Temporais III/R/air_passengersRData Portugues.RData")
df_train = subset(air_passengers, ds < "1959-01-01")
df_test = subset(air_passengers, ds >= "1959-01-01")
# Não esqueça de confirmar se o pacote "prophet" está ativo
m <- prophet(df_train,seasonality.mode = 'multiplicative')
# Vamos definir o período da previsão e a frequência (m, s, a)
future <- make_future_dataframe(m, 24, freq = 'm', include_history = F)
forecast <- predict(m, future)
plot(m, forecast)
pred <- forecast$yhat
err <- df_test$y - forecast$yhat
mape <- mean(abs(err) / (pred+ err)) * 100
rmse <- sqrt(mean(err^2, na.rm = TRUE))
mae <- mean(abs(err), na.rm = TRUE)
cbind(mape, rmse, mae)
str(uschange) # formato dos dados
head(uschange) # avaliação das 5 primeiras linhas
summary(uschange) # análise das variáveis
uschange_df <- uschange[,1:2] # subconjunto da base
autoplot(uschange_df, facets=TRUE) + # plotando os dados
xlab("Year") + ylab("") +
ggtitle("Quarterly changes in US consumptionand personal income")
# Plotando a autocorrelação, aparentemente a variável "income" é mais estacionária
ggAcf(uschange_df) # plotando o ACF
ggPacf(uschange_df) # plotando o PCF
# Decomposição
# variável "consumption"
uschange_freq_4 <- uschange_df[,"Consumption"] %>% ts(., frequency=4)
uschange_stl <- stl(uschange_freq_4, s.window = "periodic")
autoplot(uschange_stl)
# variável "income"
uschange_income_freq_4 <- uschange_df[,"Income"] %>% ts(., frequency=4)
uschange_income_stl <- stl(uschange_income_freq_4, s.window = "periodic")
autoplot(uschange_income_stl)
uschange_arimax <- auto.arima(uschange_df[,"Consumption"], # especificando a tendência
xreg = uschange_df[,"Income"], # variável exógena
trace = TRUE,
seasonal= FALSE,
stepwise=FALSE,
approximation=FALSE)
summary(uschange_arimax) # avaliação do modelo
checkresiduals(uschange_arimax) # avaliação dos resíduos do modelo
test(resid(uschange_arimax)) # testes dos resíduos do modelo
test.data.table(resid(uschange_arimax)) # testes dos resíduos do modelo
test(resid(uschange_arimax)) # testes dos resíduos do modelo
# Forecast pelo modelo SARIMAX, inclusão da sazonalidade
uschange_sarimax <- auto.arima(uschange_df[,"Consumption"], # especificação do modelo
xreg = uschange_df[,"Income"], # variável exógena
trace = TRUE,
seasonal= TRUE, # altera o argumento
stepwise=FALSE,
approximation=FALSE)
summary(uschange_sarimax) # avaliação do modelo
checkresiduals(uschange_sarimax) # p valor não significativo
# Especificando a série
uschange_ts <- ts(uschange_df, frequency = 4, start = 1970, end= 2016 )
# Separando a base em treino e teste
uschange_train <- window(uschange_ts, end=2010)
uschange_test <- window(uschange_ts, start=2011)
# Treinando o modelo apenas com dados de treino
uschange_arimax2 <- auto.arima(uschange_train[,"Consumption"], # especificando a tendência
xreg = uschange_train[,"Income"], # variável exógena
trace = FALSE, # não apresentar modelos modificados
seasonal= FALSE, # não permitir modelo SARIMAX
stepwise= FALSE,
approximation=FALSE)
# Elaborando as previsões
myforecasts <- forecast::forecast(uschange_arimax2, xreg=rep(uschange_test[,"Income"],20))
# Plotando as previsões
autoplot(myforecasts) + autolayer(uschange_ts[,"Consumption"]) + xlim(1995, 2017)
# Elaborando as previsões
myforecasts <- forecast::forecast(uschange_arimax2, xreg= rep(mean(uschange_test[,"Income"]), 25) )
# Plotando as previsões
autoplot(myforecasts) + autolayer(uschange_ts[,"Consumption"]) + xlim(1995, 2017)
pacotes <- c("prophet","tidymodels","fpp2","tidyverse","tseries","itsmr",
"timetk","modeltime","glmnet","h2o","modeltime.h2o")
if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
instalador <- pacotes[!pacotes %in% installed.packages()]
for(i in 1:length(instalador)) {
install.packages(instalador, dependencies = T)
break()}
sapply(pacotes, require, character = T)
} else {
sapply(pacotes, require, character = T)
}
load("air_passengers.RData")
load("~/GitHub/DSA211/SML Séries Temporais III/R/air_passengersRData Portugues.RData")
df_train = subset(air_passengers, ds < "1959-01-01")
df_test = subset(air_passengers, ds >= "1959-01-01")
# Não esqueça de confirmar se o pacote "prophet" está ativo
m <- prophet(df_train,seasonality.mode = 'multiplicative')
# Vamos definir o período da previsão e a frequência (m, s, a)
future <- make_future_dataframe(m, 24, freq = 'm', include_history = F)
forecast <- predict(m, future)
plot(m, forecast)
pred <- forecast$yhat
err <- df_test$y - forecast$yhat
mape <- mean(abs(err) / (pred+ err)) * 100
rmse <- sqrt(mean(err^2, na.rm = TRUE))
mae <- mean(abs(err), na.rm = TRUE)
cbind(mape, rmse, mae)
str(uschange) # formato dos dados
head(uschange) # avaliação das 5 primeiras linhas
summary(uschange) # análise das variáveis
uschange_df <- uschange[,1:2] # subconjunto da base
autoplot(uschange_df, facets=TRUE) + # plotando os dados
xlab("Year") + ylab("") +
ggtitle("Quarterly changes in US consumptionand personal income")
# Plotando a autocorrelação, aparentemente a variável "income" é mais estacionária
ggAcf(uschange_df) # plotando o ACF
ggPacf(uschange_df) # plotando o PCF
# Decomposição
# variável "consumption"
uschange_freq_4 <- uschange_df[,"Consumption"] %>% ts(., frequency=4)
uschange_stl <- stl(uschange_freq_4, s.window = "periodic")
autoplot(uschange_stl)
# variável "income"
uschange_income_freq_4 <- uschange_df[,"Income"] %>% ts(., frequency=4)
uschange_income_stl <- stl(uschange_income_freq_4, s.window = "periodic")
autoplot(uschange_income_stl)
uschange_arimax <- auto.arima(uschange_df[,"Consumption"], # especificando a tendência
xreg = uschange_df[,"Income"], # variável exógena
trace = TRUE,
seasonal= FALSE,
stepwise=FALSE,
approximation=FALSE)
summary(uschange_arimax) # avaliação do modelo
checkresiduals(uschange_arimax) # avaliação dos resíduos do modelo
test(resid(uschange_arimax)) # testes dos resíduos do modelo
# Forecast pelo modelo SARIMAX, inclusão da sazonalidade
uschange_sarimax <- auto.arima(uschange_df[,"Consumption"], # especificação do modelo
xreg = uschange_df[,"Income"], # variável exógena
trace = TRUE,
seasonal= TRUE, # altera o argumento
stepwise=FALSE,
approximation=FALSE)
summary(uschange_sarimax) # avaliação do modelo
checkresiduals(uschange_sarimax) # p valor não significativo
# Especificando a série
uschange_ts <- ts(uschange_df, frequency = 4, start = 1970, end= 2016 )
# Separando a base em treino e teste
uschange_train <- window(uschange_ts, end=2010)
uschange_test <- window(uschange_ts, start=2011)
# Treinando o modelo apenas com dados de treino
uschange_arimax2 <- auto.arima(uschange_train[,"Consumption"], # especificando a tendência
xreg = uschange_train[,"Income"], # variável exógena
trace = FALSE, # não apresentar modelos modificados
seasonal= FALSE, # não permitir modelo SARIMAX
stepwise= FALSE,
approximation=FALSE)
# Elaborando as previsões
myforecasts <- forecast::forecast(uschange_arimax2, xreg=rep(uschange_test[,"Income"],20))
# Plotando as previsões
autoplot(myforecasts) + autolayer(uschange_ts[,"Consumption"]) + xlim(1995, 2017)
# Elaborando as previsões
myforecasts <- forecast::forecast(uschange_arimax2, xreg= rep(mean(uschange_test[,"Income"]), 25) )
# Plotando as previsões
autoplot(myforecasts) + autolayer(uschange_ts[,"Consumption"]) + xlim(1995, 2017)
bike_transactions_tbl <- bike_sharing_daily %>% # carregando a base
select(dteday, cnt) %>%
set_names(c("date", "value"))
bike_transactions_tbl # avaliando a estrutura dos dados
bike_transactions_tbl %>% # plotando os dados
plot_time_series(date, value, .interactive = FALSE)
# Vamos separar a base em treino e teste, time_series_split()
splits <- bike_transactions_tbl %>% # período de 3 meses
time_series_split(assess = "3 months", cumulative = TRUE)
# Convertendo a base em treino e teste
splits %>%
tk_time_series_cv_plan() %>%
plot_time_series_cv_plan(date, value, .interactive = FALSE)
# 1. Modelos automáticos, integração do forecast e prophet
# AutoArima
model_fit_arima <- arima_reg() %>% # algoritmo e parâmetro
set_engine("auto_arima") %>% # seleção da função do pacote
fit(value ~ date, training(splits)) # inserção da coluna data como regressor
pacotes <- c("prophet","tidymodels","fpp2","tidyverse","tseries","itsmr",
"timetk","modeltime","glmnet","h2o","modeltime.h2o")
if(sum(as.numeric(!pacotes %in% installed.packages())) != 0){
instalador <- pacotes[!pacotes %in% installed.packages()]
for(i in 1:length(instalador)) {
install.packages(instalador, dependencies = T)
break()}
sapply(pacotes, require, character = T)
} else {
sapply(pacotes, require, character = T)
}
bike_transactions_tbl <- bike_sharing_daily %>% # carregando a base
select(dteday, cnt) %>%
set_names(c("date", "value"))
bike_transactions_tbl # avaliando a estrutura dos dados
bike_transactions_tbl %>% # plotando os dados
plot_time_series(date, value, .interactive = FALSE)
# Vamos separar a base em treino e teste, time_series_split()
splits <- bike_transactions_tbl %>% # período de 3 meses
time_series_split(assess = "3 months", cumulative = TRUE)
# Convertendo a base em treino e teste
splits %>%
tk_time_series_cv_plan() %>%
plot_time_series_cv_plan(date, value, .interactive = FALSE)
# 1. Modelos automáticos, integração do forecast e prophet
# AutoArima
model_fit_arima <- arima_reg() %>% # algoritmo e parâmetro
set_engine("auto_arima") %>% # seleção da função do pacote
fit(value ~ date, training(splits)) # inserção da coluna data como regressor
model_fit_arima # sumário do modelo
#Prophet
model_fit_prophet <- prophet_reg(seasonality_yearly = TRUE) %>%  # inserção da sazonalidade
set_engine("prophet") %>% # algoritmo prophet
fit(value ~ date, training(splits))
model_fit_prophet # sumário do modelo
# Adição de etapas de série temporal
recipe_spec <- recipe(value ~ date, training(splits)) %>%
step_timeseries_signature(date) %>%
step_rm(contains("am.pm"), contains("hour"), contains("minute"),
contains("second"), contains("xts")) %>%
step_fourier(date, period = 365, K = 5) %>%
step_dummy(all_nominal())
recipe_spec %>% prep() %>% juice()
# ElasticNet: regressão regularizada que combina linearmente os métodos Lasso e Ridge
model_spec_glmnet <- linear_reg(penalty = 0.01, mixture = 0.5) %>%
set_engine("glmnet")
# Vamos aplicar o workflox
workflow_fit_glmnet <- workflow() %>%
add_model(model_spec_glmnet) %>% # especificação do modelo
add_recipe(recipe_spec %>% step_rm(date)) %>% # adicionando o pré-processamento
fit(training(splits)) # ajustando o fluxo
# Random Forest: método para classificação, regressão e outras por meio da
# construção de árvores de decisão no treinamento
model_spec_rf <- rand_forest(trees = 500, min_n = 50) %>% # especificando o modelo
set_engine("randomForest")
workflow_fit_rf <- workflow() %>% # aplicando o fluxo
add_model(model_spec_rf) %>%
add_recipe(recipe_spec %>% step_rm(date)) %>%
fit(training(splits))
# Iniciando o fluxo
model_spec_prophet_boost <- prophet_boost(seasonality_yearly = TRUE) %>%
set_engine("prophet_xgboost")
workflow_fit_prophet_boost <- workflow() %>% # aplicando o fluxo
add_model(model_spec_prophet_boost) %>%
add_recipe(recipe_spec) %>%
fit(training(splits))
workflow_fit_prophet_boost # ajustando o fluxo
# Criando a tabela
model_table <- modeltime_table(
model_fit_arima,
model_fit_prophet,
workflow_fit_glmnet,
workflow_fit_rf,
workflow_fit_prophet_boost
)
# Confirme se o pacote ""glmnet" está ativo
model_table # avaliando tabela
# Calibrando
calibration_table <- model_table %>%
modeltime_calibrate(testing(splits))
calibration_table # avaliando a tabela
# Testando a previsão
calibration_table %>% # gerando os dados de previsão para os testes da tabela
modeltime_forecast(actual_data = bike_transactions_tbl) %>%
plot_modeltime_forecast(.interactive = FALSE) # plotando
# Avaliando a acurácia
calibration_table %>%
modeltime_accuracy() %>% # criando métricas de acurácia
table_modeltime_accuracy(.interactive = FALSE) # criando tabela de comparação
calibration_table %>% # Removendo o modelo ARIMA pela baixa acurácia
filter(.model_id != 1) %>%
# Refinando a previsão futura
modeltime_refit(bike_transactions_tbl) %>%
modeltime_forecast(h = "12 months", actual_data = bike_transactions_tbl) %>%
plot_modeltime_forecast(.interactive = FALSE)
data_tbl <- walmart_sales_weekly %>% # carregando a base
select(id, Date, Weekly_Sales)
# Plotando a série
data_tbl %>%
group_by(id) %>%
plot_time_series(
.date_var    = Date,
.value       = Weekly_Sales,
.facet_ncol  = 2,
.smooth      = F,
.interactive = F
)
splits <- time_series_split(data_tbl, assess = "3 month", cumulative = TRUE) # período de 3 meses
recipe_spec <- recipe(Weekly_Sales ~ ., data = training(splits)) %>%
step_timeseries_signature(Date)
train_tbl <- training(splits) %>% bake(prep(recipe_spec), .) # base de treino
test_tbl  <- testing(splits) %>% bake(prep(recipe_spec), .) # base de teste
################################################################################
################################################################################
# Para que possam implementar o h20 é preciso que tenham o Java instalado
################################################################################
# Caso não tenha o link de acesso para download é: https://java.com/en/
################################################################################
h2o.init(
nthreads = -1,
ip       = 'localhost',
port     = 54321
)
model_spec <- automl_reg(mode = 'regression') %>% # uso do algoritmo de ML
set_engine(
engine                     = 'h2o',
max_runtime_secs           = 5,
max_runtime_secs_per_model = 3,
max_models                 = 3,
nfolds                     = 5,
exclude_algos              = c("DeepLearning"),
verbosity                  = NULL,
seed                       = 786
)
model_spec # avaliando a especificação do modelo
# Treinando o modelo
model_fitted <- model_spec %>%
fit(Weekly_Sales ~ ., data = train_tbl)
# Checando o modelo
model_fitted
# Prevendo o modelo de teste
predict(model_fitted, test_tbl)
# Criando a tabela modelo
modeltime_tbl <- modeltime_table(
model_fitted
)
# Avaliando a tabela
modeltime_tbl
# Calibrando, testando e configurando a acurácia do modelo
modeltime_tbl %>%
modeltime_calibrate(test_tbl) %>%
modeltime_forecast(
new_data    = test_tbl,
actual_data = data_tbl,
keep_data   = TRUE
) %>%
group_by(id) %>%
plot_modeltime_forecast(
.facet_ncol = 2,
.interactive = FALSE
)
# Fazendo a previsão
data_prepared_tbl <- bind_rows(train_tbl, test_tbl) # preparando os dados
future_tbl <- data_prepared_tbl %>% # criando o dataset
group_by(id) %>%
future_frame(.length_out = "1 year") %>%
ungroup()
future_prepared_tbl <- bake(prep(recipe_spec), future_tbl)
# Retreinando todo a base de dados. Prática para melhorar o resultado das previsões
refit_tbl <- modeltime_tbl %>%
modeltime_refit(data_prepared_tbl)
# Visualiação da previsão
refit_tbl %>%
modeltime_forecast(
new_data    = future_prepared_tbl,
actual_data = data_prepared_tbl,
keep_data   = TRUE
) %>%
group_by(id) %>%
plot_modeltime_forecast(
.facet_ncol  = 2,
.interactive = FALSE
)
